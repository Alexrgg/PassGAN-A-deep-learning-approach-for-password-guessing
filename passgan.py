# -*- coding: utf-8 -*-
"""PassGAN.ipynb

Automatically generated by Colaboratory.



There are some bugs in latest versions so downgrading.
"""

!pip install pytorch-lightning==0.8.5
!pip install torch==1.4.0 torchvision==0.5.0

"""***Dataloader starts***"""

import torch
import copy
import numpy as np

!mkdir data
!curl -L -o data/train.txt https://github.com/brannondorsey/PassGAN/releases/download/data/rockyou-train.txt

from google.colab import drive
drive.mount('/content/drive')

! cat data/train.txt | head -n 10

def load_dataset(path, max_length, tokenize=False, max_vocab_size=2048, seed = 2020):
    
    lines = []

    with open(path, 'r', encoding="ISO-8859-1") as f:
        for line in f:
            line = line[:-1]
            if tokenize:
                line = tokenize_string(line)
            else:
                line = tuple(line)

            if len(line) > max_length:
                line = line[:max_length]
                continue # don't include this sample, its too long

            # right pad with ` character
            lines.append(line + ( ("`",)*(max_length-len(line)) ) )

    lines = list(set(lines))# Removing duplictes so that no common element in training and validation
    np.random.seed(seed)

    np.random.shuffle(lines)

    import collections
    counts = collections.Counter(char for line in lines for char in line)

    charmap = {'unk':0}
    inv_charmap = ['unk']

    for char,count in counts.most_common(max_vocab_size-1):
        if char not in charmap:
            charmap[char] = len(inv_charmap)
            inv_charmap.append(char)

    filtered_lines = []
    for line in lines:
        filtered_line = []
        for char in line:
            if char in charmap:
                filtered_line.append(char)
            else:
                filtered_line.append('unk')
        filtered_lines.append(tuple(filtered_line))

    # for i in xrange(100):
    #     print filtered_lines[i]

    print ("loaded {} lines in dataset".format(len(lines)))
    return filtered_lines, charmap, inv_charmap

class PasswordDataset(torch.utils.data.Dataset):
    def __init__(self, file_path, max_length = 10):
        self.lines, self.charmap, self.inv_charmap = load_dataset(file_path, max_length)

    def __len__(self):
        return len(self.lines)

    def __getitem__(self, idx):
        tokenized = torch.tensor([self.charmap[c] for c in self.lines[idx]], dtype = torch.int64) #will shift to precomputed if needed to improve speed
        one_hot = torch.nn.functional.one_hot(tokenized, 2048) # size=(max_length,vocab_size)

        return one_hot
    def get_charmap(self):
        return copy.deepcopy(self.charmap)
    
    def get_inv_charmap(self):
        return copy.deepcopy(self.inv_charmap)

dataset = PasswordDataset('./data/train.txt') # This takes some time to run

print(dataset[100])

import os
from argparse import ArgumentParser
from collections import OrderedDict

import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
import torchvision
import torchvision.transforms as transforms
from torch.utils.data import DataLoader
from torchvision.datasets import MNIST

from pytorch_lightning.core import LightningModule
from pytorch_lightning.trainer import Trainer
from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping, LearningRateLogger
import copy

"""Helper functions :)

"""

def make_some_noise(shape):
    return torch.randn(shape)

class resblock(nn.Module):
    def __init__(self, channels):
        super().__init__()
        self.channels = channels
        self.block = nn.Sequential(
                        nn.ReLU(True),
                        nn.Conv1d(self.channels, self.channels, 5, stride=1, bias=True, padding=2),
                        nn.ReLU(True),
                        nn.Conv1d(self.channels, self.channels, 5, stride=1, bias=True, padding=2))
                        
        
    def forward(self, x):
        return x + 0.3*self.block(x)

"""Generator

"""

class Generator(nn.Module):
    def __init__(self, latent_dim, seq_len, layer_dim, vocab_len):
        super().__init__()
        self.noise_dim = latent_dim
        self.seq_len = seq_len
        self.layer_dim = layer_dim
        self.output_dim = vocab_len
        
        self.linear = nn.Linear(self.noise_dim, self.seq_len*self.layer_dim, bias=True)
        self.resblock_1 = resblock(self.layer_dim)
        self.resblock_2 = resblock(self.layer_dim)
        self.resblock_3 = resblock(self.layer_dim)
        self.resblock_4 = resblock(self.layer_dim)
        self.resblock_5 = resblock(self.layer_dim)
        self.conv = nn.Conv1d(self.layer_dim, self.output_dim, 1, bias=True)
        self.softmax = nn.Softmax(dim=2) 
        
    def forward(self, inp):
        x = inp
        
        x = self.linear(x)
        x = torch.reshape(x, (-1, self.layer_dim, self.seq_len))
        
        x = self.resblock_1(x)
        x = self.resblock_2(x)
        x = self.resblock_3(x)
        x = self.resblock_4(x)
        x = self.resblock_5(x)
        
        x = self.conv(x)
        x = torch.transpose(x,1,2)
        x = self.softmax(torch.exp(x))
        
        return x

"""Discriminator"""

class Discriminator(nn.Module):
    def __init__(self,seq_len, layer_dim, vocab_len):
        super().__init__()
        self.seq_len = seq_len
        self.layer_dim = layer_dim
        self.inp_dim = vocab_len
            
        self.conv = nn.Conv1d(self.inp_dim, self.layer_dim, 1, bias=True)
        self.resblock_1 = resblock(self.layer_dim)
        self.resblock_2 = resblock(self.layer_dim)
        self.resblock_3 = resblock(self.layer_dim)
        self.resblock_4 = resblock(self.layer_dim)
        self.resblock_5 = resblock(self.layer_dim)
        
        self.linear = nn.Linear(self.layer_dim * self.seq_len, 1, bias=True)
        
    def forward(self,inp):
        x = inp    
        x = torch.transpose(x, 1, 2)
        x = self.conv(x)
        
        x = self.resblock_1(x)
        x = self.resblock_2(x)
        x = self.resblock_3(x)
        x = self.resblock_4(x)
        x = self.resblock_5(x)
            
        x = torch.reshape(x, (-1, self.layer_dim * self.seq_len))
        x = self.linear(x)
            
        return x

"""Below code handles the training only."""

class GAN(LightningModule):

    def __init__(self, hparams):
        super().__init__()
        self.hparams = hparams

        self.generator = Generator(latent_dim=hparams.latent_dim, seq_len=hparams.seq_len, layer_dim=hparams.layer_dim, vocab_len=hparams.vocab_len)
        self.discriminator = Discriminator(seq_len=hparams.seq_len, layer_dim=hparams.layer_dim, vocab_len=hparams.vocab_len)

        # cache for generated images
        self.generated_imgs = None
        self.last_imgs = None

    def forward(self, z):
        return self.generator(z)

    def adversarial_loss(self,D, real_samples, fake_samples):
        """Calculates the gradient penalty loss for WGAN GP"""
        # Random weight term for interpolation between real and fake samples


        alpha = torch.rand((real_samples.size()[0], 1, 1)).cuda()
        # Get random interpolation between real and fake samples
        interpolates = (alpha * real_samples + ((1 - alpha) * fake_samples)).requires_grad_(True)
        d_interpolates = D(interpolates)
        fake = torch.ones_like(d_interpolates).requires_grad_(False)
        # Get gradient w.r.t. interpolates
        gradients = torch.autograd.grad(
            outputs=d_interpolates,
            inputs=interpolates,
            grad_outputs=fake,
            create_graph=True,
            retain_graph=True,
        )[0]
        gradients = gradients.contiguous().view(gradients.size()[0], -1)
        gradient_penalty = ((gradients.norm(2, dim=1) - 1) ** 2).mean()
        return gradient_penalty

    def training_step(self, batch, batch_idx, optimizer_idx):
        imgs = batch.float()
        self.last_imgs = imgs

        # train generator
        if optimizer_idx == 0:
            # sample noise
            z = torch.randn(imgs.shape[0], self.hparams.latent_dim)
            z = z.type_as(imgs).cuda()

            # generate images
            self.generated_imgs = self(z)

            # log sampled images
            # sample_imgs = self.generated_imgs[:6]
            # grid = torchvision.utils.make_grid(sample_imgs)
            # self.logger.experiment.add_image('generated_images', grid, 0)

            # ground truth result (ie: all fake)
            # put on GPU because we created this tensor inside training_loop

            # adversarial loss is binary cross-entropy

            g_loss = -self.discriminator(self.generated_imgs).mean()

            tqdm_dict = {'g_loss': g_loss}
            output = OrderedDict({
                'loss': g_loss,
                'progress_bar': tqdm_dict,
                'log': tqdm_dict
            })
            return output

        # train discriminator
        if optimizer_idx == 1:
            # Measure discriminator's ability to classify real from generated samples

            d_loss = self.adversarial_loss(self.discriminator, imgs, self.generated_imgs.detach().clone()) - self.discriminator(imgs).mean() + self.discriminator(self.generated_imgs.detach().clone()).mean()

            tqdm_dict = {'d_loss': d_loss}
            output = OrderedDict({
                'loss': d_loss,
                'progress_bar': tqdm_dict,
                'log': tqdm_dict
            })
            return output

    def configure_optimizers(self):
        lr = self.hparams.lr
        b1 = self.hparams.b1
        b2 = self.hparams.b2

        opt_g = torch.optim.Adam(self.generator.parameters(), lr=lr, betas=(b1, b2))
        opt_d = torch.optim.Adam(self.discriminator.parameters(), lr=lr, betas=(b1, b2))
        n_critic = 10
        return (
            {'optimizer': opt_g, 'frequency': 1},

            {'optimizer': opt_d, 'frequency': n_critic},
        )
    def __dataloader(self):


        n_val = int(len(dataset) * 0.2)
        n_train = len(dataset) - n_val

        train_ds=torch.utils.data.Subset(dataset,range(0,n_train))
        val_ds=torch.utils.data.Subset(dataset,range(n_train,len(dataset)))

        train_loader = DataLoader(train_ds, batch_size=self.hparams.batch_size,num_workers=4, pin_memory=True, shuffle=True, drop_last = True)
        val_loader = DataLoader(val_ds, batch_size=1,num_workers=4, pin_memory=True, shuffle=False, drop_last = True)

        return {
            'train': train_loader,
            'val': val_loader,
        }

    def train_dataloader(self):
        return self.__dataloader()['train']

def main(hparams, model = None):
    # ------------------------
    # 1 INIT LIGHTNING MODEL
    # ------------------------
    if model == None:
      model = GAN(hparams)

    # ------------------------
    # 2 INIT TRAINER
    # ------------------------
    os.makedirs(hparams.log_dir, exist_ok=True)
    try:
        log_dir = sorted(os.listdir(hparams.log_dir))[-1]
    except IndexError:
        log_dir = os.path.join(hparams.log_dir, 'version_0')

    checkpoint_callback = ModelCheckpoint(
        filepath=os.path.join(log_dir, 'checkpoints'),
        save_top_k=-1,
        verbose=True,
    )
    trainer = Trainer(gpus = 1,limit_train_batches=1.0, max_epochs=5, benchmark=True,
                      checkpoint_callback = checkpoint_callback)

    # ------------------------
    # 3 START TRAINING
    # ------------------------
    trainer.fit(model)

    return  model

parser = ArgumentParser()
parser.add_argument('--log_dir', default='lightning_logs')
parser.add_argument("--batch_size", type=int, default=64, help="size of the batches")
parser.add_argument("--lr", type=float, default=0.0001, help="adam: learning rate")
parser.add_argument("--b1", type=float, default=0.5,
                    help="adam: decay of first order momentum of gradient")
parser.add_argument("--b2", type=float, default=0.9,
                    help="adam: decay of first order momentum of gradient")
parser.add_argument("--latent_dim", type=int, default=128,
                    help="dimensionality of the latent space")
parser.add_argument("--seq_len", type=int, default=10,
                    help="length of output sequence")
#model = None
parser.add_argument("--vocab_len", type=int, default=2048,
                    help="size of vocabulary")
parser.add_argument("--layer_dim", type=int, default=128,
                    help="layer dimension for both generator and discriminator")
hparams = parser.parse_args(args=[])

model = GAN(hparams)
checkpoint = torch.load('/content/drive/My Drive/PassGAN/Model.tar')
model.load_state_dict(checkpoint['model_state_dict'])

model = main(hparams,model)

def generate_samples(model, inv_charmap, latent_dim, n_samples):
    samples = []
    for _ in range(n_samples):
        probs = model(make_some_noise(latent_dim))
        preds = probs.squeeze(0).argmax(axis = 1)
        char_preds = [inv_charmap[x] for x in preds]
        samples.append(char_preds)
    return samples

inv_charmap = dataset.get_inv_charmap()

samples = generate_samples(model, inv_charmap, hparams.latent_dim, 10000)

print(samples[0])

val_loader = model._GAN__dataloader()['val']

def score_model(model, val_loader, samples):
    samples_set = set(tuple(x) for x in samples)
    matched = 0
    
    for ind, data in enumerate(val_loader):
        preds = (data.squeeze(0).argmax(axis=1))
        char_preds = tuple([inv_charmap[x] for x in preds])
    
        if char_preds in samples_set:
           matched+=1
        if ind %10000 == 9999:
            print(ind)
        if ind % 1000000 == 999999:
            break
    print(matched)
    return

score_model(model, val_loader, samples)

out =model(make_some_noise(128))

preds = out.squeeze(0).argmax(axis = 1)

preds

real_preds = [inv_charmap[x] for x in preds]
print(real_preds)
